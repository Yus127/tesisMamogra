{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be testing biomed clip with a little set of images, just searching the elements of calcifications and nodules (maybe include the birads) to make the radiological report. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from open_clip import create_model_from_pretrained, get_tokenizer # works on open-clip-torch>=2.23.0, timm>=0.9.8\n",
    "tokenizer = get_tokenizer('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AdamW\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nrrd\n",
    "from torchvision import transforms\n",
    "from typing import List, Optional\n",
    "from transformers import TrainingArguments, Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_biomed_clip(model, train_dataloader, num_epochs, device, learning_rate=5e-5):\n",
    "    \"\"\"\n",
    "    Manual training loop for BioMed CLIP \n",
    "    THIS CODE HAS NOT BEEN TESTED\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch in train_dataloader:\n",
    "            try:\n",
    "                # Move batch to device\n",
    "                images = batch['image'].to(device)\n",
    "                texts = batch['text']\n",
    "                \n",
    "                if isinstance(texts, dict):\n",
    "                    texts = {k: v.to(device) for k, v in texts.items()}\n",
    "                else:\n",
    "                    texts = texts.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(image=images, text=texts)\n",
    "                loss = outputs.loss if hasattr(outputs, 'loss') else outputs[0]\n",
    "                \n",
    "                # Backward pass and optimization\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Accumulate loss\n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "                \n",
    "                if num_batches % 10 == 0:\n",
    "                    print(f\"Epoch {epoch+1}/{num_epochs}, Batch {num_batches}, Loss: {loss.item():.4f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in batch: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # Calculate and print average loss for the epoch\n",
    "        avg_loss = total_loss / num_batches\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, processor, image_paths, entity_info=None, max_length=100):\n",
    "    \"\"\"\n",
    "    Generate text descriptions from medical images using BioMedCLIP.\n",
    "    THIS CODE HAS NOT BEEN TESTED, I DONT KNOW IF IT WORKS\n",
    "    \"\"\"\n",
    "    # Load and process all images\n",
    "    images = [Image.open(path) for path in image_paths]\n",
    "    \n",
    "    # Process images with the BioMedCLIP processor\n",
    "    inputs = processor(images, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    # Move inputs to the same device as model\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate text without gradient computation\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_length=max_length)\n",
    "    \n",
    "    # Decode the generated text\n",
    "    generated_text = processor.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "    \n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_pl import MyDatamodule\n",
    "from model_pl import MyModel\n",
    "import lightning as L\n",
    "\n",
    "from transformers import AdamW\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from open_clip import create_model_from_pretrained, get_tokenizer\n",
    "\n",
    "\"\"\"\n",
    "BioMedCLIP Setup and Data Loading Script\n",
    "\n",
    "This script initializes the BioMedCLIP model, preprocessor, and tokenizer,\n",
    "then sets up the dataset and dataloader for training or inference.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Initialize BioMedCLIP model and preprocessor\n",
    "model, preprocess = create_model_from_pretrained('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = get_tokenizer('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')\n",
    "\n",
    "# Configuration parameters\n",
    "CONFIG = {\n",
    "    'batch_size': 32,\n",
    "    'num_workers': 4,\n",
    "    'model_name': 'microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224',\n",
    "    'data_dir': \"/Users/YusMolina/Documents/tesis/biomedCLIP/data/datosMex/\",\n",
    "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "}\n",
    "\n",
    "# Create datamodule instance\n",
    "'''\n",
    "En vez de crear un dataset, creamos una instancia del datamodule, que contendrá el dataset y los dataloaders.\n",
    "dataset = ComplexMedicalDataset(\n",
    "    data_dir=\"/Users/YusMolina/Documents/tesis/biomedCLIP/data/datosMex/\",\n",
    "    processor=preprocess,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=32, \n",
    "    shuffle=True, \n",
    "    collate_fn=ComplexMedicalDataset.collate_fn\n",
    "    )\n",
    "'''\n",
    "datamodule = MyDatamodule(\n",
    "    data_dir=\"/Users/YusMolina/Documents/tesis/biomedCLIP/data/datosMex/\",\n",
    "    processor=preprocess,\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=32,\n",
    "    num_workers=4)\n",
    "\n",
    "# Create an instance of MyModel\n",
    "model = MyModel(\n",
    "    model=model,\n",
    "    optimizer=AdamW(model.parameters(), lr=5e-5),\n",
    "    loss=\"<TODO: Loss function here>\"\n",
    "    )\n",
    "\n",
    "trainer = L.trainer()\n",
    "\n",
    "trainer.train(model, datamodule)\n",
    "trainer.eval(model, datamodule)\n",
    "\n",
    "\"\"\"\n",
    "Expected directory structure:\n",
    "/data/datosMex/\n",
    "    ├── dataset_info.json\n",
    "    ├── images/\n",
    "    │   ├── image1.jpg\n",
    "    │   ├── image2.jpg\n",
    "    │   └── ...\n",
    "    └── reports/\n",
    "        ├── report1.txt\n",
    "        └── ...\n",
    "\n",
    "dataset_info.json format:\n",
    "{\n",
    "    \"case_id\": {\n",
    "        \"image_paths\": [\"path/to/image1.jpg\", ...],\n",
    "        \"report\": \"medical report text\"\n",
    "    },\n",
    "    ...\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Optional: Print configuration for verification\n",
    "def print_setup_info():\n",
    "    \"\"\"Print configuration and setup information for verification.\"\"\"\n",
    "    print(\"\\nBioMedCLIP Setup Information:\")\n",
    "    print(f\"Device: {CONFIG['device']}\")\n",
    "    print(f\"Dataset size: {len(dataset)}\")\n",
    "    print(f\"Number of batches: {len(dataloader)}\")\n",
    "    print(f\"Batch size: {CONFIG['batch_size']}\")\n",
    "    print(f\"Number of workers: {CONFIG['num_workers']}\")\n",
    "    print(\"\\nModel Information:\")\n",
    "    print(f\"Model name: {CONFIG['model_name']}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "taken from the tutorial: https://huggingface.co/docs/transformers/training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute evaluation metrics for CLIP-like models, specifically designed for\n",
    "    image-text matching tasks.\n",
    "    \n",
    "    NOT TESTED\n",
    "    \"\"\"\n",
    "    outputs = eval_pred.predictions\n",
    "    \n",
    "    # Handle CLIP-style embeddings (image_embeds, text_embeds)\n",
    "    if isinstance(outputs, tuple):\n",
    "        image_embeds, text_embeds = outputs\n",
    "        \n",
    "        # Compute cosine similarity matrix\n",
    "        similarity = torch.matmul(\n",
    "            torch.from_numpy(image_embeds), \n",
    "            torch.from_numpy(text_embeds).t()\n",
    "        )\n",
    "        \n",
    "        # Get predictions (diagonal elements should be highest for correct pairs)\n",
    "        predictions = torch.argmax(similarity, dim=1).numpy()\n",
    "        labels = np.arange(len(predictions))  # Assuming paired data\n",
    "        \n",
    "        # Calculate evaluation metrics\n",
    "        accuracy = (predictions == labels).mean()\n",
    "        mean_similarity = similarity.diagonal().mean().item()\n",
    "        \n",
    "        # Return dictionary of metrics\n",
    "        return {\n",
    "            \"accuracy\": accuracy,                    # Matching accuracy\n",
    "            \"mean_similarity\": mean_similarity       # Average similarity for correct pairs\n",
    "        }\n",
    "    \n",
    "    # Handle standard classification outputs\n",
    "    else:\n",
    "        predictions = np.argmax(outputs, axis=-1)\n",
    "        labels = eval_pred.label_ids\n",
    "        accuracy = (predictions == labels).mean()\n",
    "        return {\"accuracy\": accuracy}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I know is bad, sorry\n",
    "train_dataset = dataset\n",
    "eval_dataset = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments setup\n",
    "def setup_training(\n",
    "    output_dir: str = \"./results\",\n",
    "    num_epochs: int = 3,\n",
    "    eval_steps: int = 100,\n",
    "    learning_rate: float = 5e-5\n",
    ") -> TrainingArguments:\n",
    "   \n",
    "    return TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=num_epochs,        \n",
    "        eval_steps=eval_steps,\n",
    "        learning_rate=learning_rate,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=10,\n",
    "        report_to=\"tensorboard\",\n",
    "        # Gradient related\n",
    "        gradient_accumulation_steps=1,\n",
    "        weight_decay=0.01,\n",
    "        # Mixed precision training\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        # Other settings\n",
    "        remove_unused_columns=False,  # Important for CLIP-like models\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = setup_training(output_dir=\"./results\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset= train_dataset,\n",
    "    eval_dataset= eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prints to obain more information of the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(model.forward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchinfo\n",
    "\n",
    "torchinfo.summary(model, input_size=(4, 3, 224, 224))  # specify input size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tesis3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
